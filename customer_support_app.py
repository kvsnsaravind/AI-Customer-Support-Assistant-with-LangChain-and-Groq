# -*- coding: utf-8 -*-
"""Customer-Support App.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mqFl57ye1TSnViJEBWQqDxIe1EEQXuu3

Customer-Support App using Llama and Gorq model
"""

!pip install python-dotenv langchain[groq]

from google.colab import userdata
import os

os.environ['GROQ_API_KEY'] = userdata.get('groqAPIKey')

from langchain.chat_models import init_chat_model

llm = init_chat_model("llama3-8b-8192", model_provider="groq")

llm.invoke("What is langchain in one sentence")

!pip install langchain langchain_core langchain_groq langchain_community langgraph

from typing_extensions import TypedDict, Dict
from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables.graph import MermaidDrawMethod
from IPython.display import display , Image

from typing import TypedDict
class State(TypedDict):
  query: str
  category: str
  sentiment: str
  response : str

def categorize(state: State) -> State:
  prompt = ChatPromptTemplate.from_template("Categorize the following query into one of the following categories: "
  "Technical, Billing, General. Query: {query}")
  chain = prompt | llm
  category = chain.invoke({"query": state["query"]}).content
  print(f"\n{category}")
  return {"category": category}

def analyze_sentiment(state: State) -> State:
  print("\nAnalyzing the sentiment : ")
  prompt = ChatPromptTemplate.from_template(
      "Analyze the sentiment of the following customer query and ans in word"
      "Response with either 'Positive', 'Neutral' , or 'Negative'. Query: {query}"
  )
  chain = prompt | llm
  sentiment = chain.invoke({"query": state["query"]}).content
  print(f"\nSentiment : {sentiment}")
  return {"sentiment": sentiment}

def handle_technical(state: State)->State:
  print("\nHandling the technical query")
  prompt = ChatPromptTemplate.from_template(
      "Provide a technical support response to the following query : {query}"
  )
  chain = prompt | llm
  response = chain.invoke({"query": state["query"]}).content
  return {"response": response}

def handle_billing(state: State)->State:
  print("\nHandling the billing query")
  prompt = ChatPromptTemplate.from_template(
      "Provide a billing support response to the following query : {query}"
  )
  chain = prompt | llm
  response = chain.invoke({"query": state["query"]}).content
  return {"response": response}

def handle_general(state: State)->State:
  print("\nHandling the general query")
  prompt = ChatPromptTemplate.from_template(
      "Provide a general support response to the following query : {query}"
  )
  chain = prompt | llm
  response = chain.invoke({"query": state["query"]}).content
  return {"response": response}

def escalate(state: State)->State:
  print("Escalating Query")
  return {"response": "This query has been escalate to a human agent due to its negative sentiment"}


def route_query(state: State)->State:
  if "negative" in state["sentiment"].lower(): # if state["sentiment"] == "Negative":
    return "escalate"
  elif state["category"] == "Technical":
    return "handle_technical"
  elif state["category"] == "Billing":
    return "handle_billing"
  else:
    return "handle_general"

workflow = StateGraph(State)

workflow.add_node("categorize", categorize)
workflow.add_node("analyze_sentiment", analyze_sentiment)
workflow.add_node("handle_technical", handle_technical)
workflow.add_node("handle_billing", handle_billing)
workflow.add_node("handle_general", handle_general)
workflow.add_node("escalate", escalate)

workflow.add_conditional_edges("analyze_sentiment", route_query)

workflow.add_edge("categorize", "analyze_sentiment")
workflow.add_edge("handle_technical", END)
workflow.add_edge("handle_billing", END)
workflow.add_edge("handle_general", END)
workflow.add_edge("escalate", END)

workflow.set_entry_point("categorize")

app = workflow.compile()

from IPython.display import Image, display

try:
    display(Image(app.get_graph().draw_mermaid_png()))
except Exception:
    # This requires some extra dependencies and is optional
    pass

def run_customer_support(query: str) -> Dict[str, str]:
  results = app.invoke({"query": query})
  return {
      "category":results['category'],
      "sentiment":results['sentiment'],
      "response": results['response']
  }

query = "The price value in my invoice is wrong"
result = run_customer_support(query)
print("\n\n\nComputed Result")
print(f"Query: {query}")
print(f"\n\nCategory: {result['category']}")
print(f"\n\nSentiment: {result['sentiment']}")
print(f"\n\nResponse: {result['response']}")
print("\n")

query = "When is the new iphone launch?"
result = run_customer_support(query)
print("\n\n\nComputed Result")
print(f"Query: {query}")
print(f"\n\nCategory: {result['category']}")
print(f"\n\nSentiment: {result['sentiment']}")
print(f"\n\nResponse: {result['response']}")
print("\n")

